25-12-09 21:40:50.280 :   task: student_C_v1_delayed_x4
  model: plain
  gpu_ids: [0]
  dist: False
  scale: 4
  n_channels: 3
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netE: None
    pretrained_netTeacher: model_zoo/swinir/001_classicalSR_DF2K_s64w8_SwinIR-M_x4.pth
    task: superresolution\student_C_v1_delayed_x4
    log: superresolution\student_C_v1_delayed_x4
    options: superresolution\student_C_v1_delayed_x4\options
    models: superresolution\student_C_v1_delayed_x4\models
    images: superresolution\student_C_v1_delayed_x4\images
    pretrained_netP: None
    pretrained_optimizerG: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/trainH/DIV2K/DIV2K_train_HR
      dataroot_L: trainsets/trainH/DIV2K/DIV2K_train_LR_bicubic/X4
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 0
      dataloader_batch_size: 32
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/Set5/HR
      dataroot_L: testsets/Set5/LR_bicubic/X4
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: swinir_student
    upscale: 4
    in_chans: 3
    img_size: 64
    window_size: 8
    img_range: 1.0
    depths: [4, 4, 4, 4]
    embed_dim: 60
    num_heads: [6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 4
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    distillation_type: feature
    distill_lossfn_type: l1
    distill_lossfn_weight: 1.0
    feature_lossfn_type: l1
    feature_lossfn_weight: 0.01
    start_feature_iter: 4000
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [10000, 16000, 18000, 19000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 2000
    checkpoint_save: 2000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/swinir/train_swinir_student_distill_feature.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: False
  use_static_graph: False
  num_gpu: 1
  rank: 0
  world_size: 1

25-12-09 21:40:50.296 : Number of train images: 900, iters: 29
25-12-09 21:40:53.439 : 
Networks name: SwinIR_Student
Params number: 988959
Net structure:
SwinIR_Student(
  (conv_first): Conv2d(3, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=60, input_resolution=(64, 64), depth=4
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.007)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.013)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.020)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=60, input_resolution=(64, 64), depth=4
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.027)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.033)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.040)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.047)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=60, input_resolution=(64, 64), depth=4
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.053)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.060)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.067)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.073)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=60, input_resolution=(64, 64), depth=4
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.080)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.087)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.093)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.100)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
    (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

25-12-09 21:40:53.558 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.005 | -0.192 |  0.192 |  0.111 | torch.Size([60, 3, 3, 3]) || conv_first.weight
 |  0.008 | -0.189 |  0.189 |  0.110 | torch.Size([60]) || conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.066 |  0.067 |  0.021 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.071 |  0.077 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.001 | -0.064 |  0.076 |  0.019 | torch.Size([60, 60]) || layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.081 |  0.070 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.001 | -0.071 |  0.066 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.067 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.072 |  0.077 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.001 | -0.064 |  0.075 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.078 |  0.077 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.071 |  0.082 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.060 |  0.072 |  0.019 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.082 |  0.077 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 | -0.001 | -0.074 |  0.061 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.068 |  0.072 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.078 |  0.081 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.059 |  0.061 |  0.019 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.068 |  0.079 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.062 |  0.064 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.norm2.bias
 |  0.001 | -0.080 |  0.068 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.078 |  0.077 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([60, 60, 3, 3]) || layers.0.conv.weight
 |  0.000 | -0.040 |  0.042 |  0.023 | torch.Size([60]) || layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.058 |  0.058 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.072 |  0.073 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.071 |  0.074 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.075 |  0.078 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.075 |  0.078 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.059 |  0.070 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.068 |  0.073 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.066 |  0.068 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.070 |  0.074 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.073 |  0.070 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.059 |  0.058 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.098 |  0.084 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.070 |  0.082 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.080 |  0.076 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.076 |  0.082 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.059 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.075 |  0.090 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.068 |  0.080 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.076 |  0.075 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.074 |  0.068 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([60, 60, 3, 3]) || layers.1.conv.weight
 | -0.003 | -0.043 |  0.040 |  0.024 | torch.Size([60]) || layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.069 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.075 |  0.076 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.078 |  0.081 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.086 |  0.074 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.075 |  0.075 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.053 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.078 |  0.072 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.070 |  0.080 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.082 |  0.076 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.083 |  0.075 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.070 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.073 |  0.086 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.078 |  0.067 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.080 |  0.082 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.067 |  0.072 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.072 |  0.054 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.084 |  0.081 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.075 |  0.071 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.067 |  0.068 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.080 |  0.070 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([60, 60, 3, 3]) || layers.2.conv.weight
 |  0.004 | -0.039 |  0.042 |  0.026 | torch.Size([60]) || layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.070 |  0.079 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.071 |  0.072 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.069 |  0.064 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.080 |  0.102 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.090 |  0.085 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.075 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.075 |  0.069 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.075 |  0.060 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.070 |  0.085 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.070 |  0.094 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.065 |  0.067 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.080 |  0.077 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.080 |  0.068 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.070 |  0.063 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.078 |  0.071 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.060 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.070 |  0.079 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.067 |  0.072 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.073 |  0.073 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.072 |  0.083 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([60, 60, 3, 3]) || layers.3.conv.weight
 | -0.001 | -0.038 |  0.039 |  0.023 | torch.Size([60]) || layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || norm.bias
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([60, 60, 3, 3]) || conv_after_body.weight
 |  0.003 | -0.040 |  0.042 |  0.025 | torch.Size([60]) || conv_after_body.bias
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 60, 3, 3]) || conv_before_upsample.0.weight
 | -0.002 | -0.036 |  0.041 |  0.020 | torch.Size([64]) || conv_before_upsample.0.bias
 |  0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.0.weight
 | -0.003 | -0.042 |  0.041 |  0.025 | torch.Size([256]) || upsample.0.bias
 |  0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.2.weight
 |  0.000 | -0.041 |  0.042 |  0.024 | torch.Size([256]) || upsample.2.bias
 |  0.000 | -0.042 |  0.042 |  0.024 | torch.Size([3, 64, 3, 3]) || conv_last.weight
 |  0.029 |  0.011 |  0.040 |  0.016 | torch.Size([3]) || conv_last.bias

25-12-09 21:54:25.822 : <epoch:  7, iter:     200, lr:2.000e-04> l_g_L1: 5.117e-02 l_g_distill: 3.930e-02 l_g_feature: 9.173e-02 G_loss: 1.822e-01 
25-12-09 22:05:11.547 : <epoch: 14, iter:     400, lr:2.000e-04> l_g_L1: 3.474e-02 l_g_distill: 2.785e-02 l_g_feature: 4.402e-02 G_loss: 1.066e-01 
25-12-09 22:16:13.355 : <epoch: 21, iter:     600, lr:2.000e-04> l_g_L1: 3.247e-02 l_g_distill: 2.345e-02 l_g_feature: 2.895e-02 G_loss: 8.486e-02 
25-12-09 22:29:17.115 : <epoch: 28, iter:     800, lr:2.000e-04> l_g_L1: 3.906e-02 l_g_distill: 2.514e-02 l_g_feature: 1.765e-02 G_loss: 8.185e-02 
25-12-09 22:42:09.879 : <epoch: 35, iter:   1,000, lr:2.000e-04> l_g_L1: 3.431e-02 l_g_distill: 2.088e-02 l_g_feature: 1.192e-02 G_loss: 6.711e-02 
25-12-09 22:54:57.381 : <epoch: 42, iter:   1,200, lr:2.000e-04> l_g_L1: 3.619e-02 l_g_distill: 2.321e-02 l_g_feature: 9.362e-03 G_loss: 6.877e-02 
25-12-09 23:07:52.763 : <epoch: 49, iter:   1,400, lr:2.000e-04> l_g_L1: 3.073e-02 l_g_distill: 2.112e-02 l_g_feature: 7.026e-03 G_loss: 5.888e-02 
25-12-09 23:21:05.869 : <epoch: 57, iter:   1,600, lr:2.000e-04> l_g_L1: 3.156e-02 l_g_distill: 2.095e-02 l_g_feature: 6.156e-03 G_loss: 5.866e-02 
25-12-09 23:34:16.057 : <epoch: 64, iter:   1,800, lr:2.000e-04> l_g_L1: 3.189e-02 l_g_distill: 2.095e-02 l_g_feature: 4.891e-03 G_loss: 5.773e-02 
25-12-09 23:46:59.422 : <epoch: 71, iter:   2,000, lr:2.000e-04> l_g_L1: 3.496e-02 l_g_distill: 2.496e-02 l_g_feature: 4.826e-03 G_loss: 6.474e-02 
25-12-09 23:46:59.422 : Saving the model.
25-12-09 23:47:03.624 : ---1--> babyx4.png | 30.62dB
25-12-09 23:47:03.719 : ---2--> birdx4.png | 28.38dB
25-12-09 23:47:03.773 : ---3--> butterflyx4.png | 21.68dB
25-12-09 23:47:03.843 : ---4--> headx4.png | 28.44dB
25-12-09 23:47:03.912 : ---5--> womanx4.png | 25.89dB
25-12-09 23:47:04.391 : <epoch: 71, iter:   2,000, Average PSNR : 27.00dB

25-12-10 00:00:04.389 : <epoch: 78, iter:   2,200, lr:2.000e-04> l_g_L1: 3.052e-02 l_g_distill: 1.697e-02 l_g_feature: 3.861e-03 G_loss: 5.135e-02 
25-12-10 00:13:11.487 : <epoch: 85, iter:   2,400, lr:2.000e-04> l_g_L1: 3.259e-02 l_g_distill: 2.266e-02 l_g_feature: 3.816e-03 G_loss: 5.907e-02 
25-12-10 00:23:45.632 : <epoch: 92, iter:   2,600, lr:2.000e-04> l_g_L1: 3.535e-02 l_g_distill: 1.942e-02 l_g_feature: 3.661e-03 G_loss: 5.844e-02 
25-12-10 00:33:52.048 : <epoch: 99, iter:   2,800, lr:2.000e-04> l_g_L1: 3.542e-02 l_g_distill: 2.176e-02 l_g_feature: 3.512e-03 G_loss: 6.070e-02 
25-12-10 00:43:55.723 : <epoch:107, iter:   3,000, lr:2.000e-04> l_g_L1: 3.289e-02 l_g_distill: 2.092e-02 l_g_feature: 3.152e-03 G_loss: 5.696e-02 
25-12-10 00:53:59.455 : <epoch:114, iter:   3,200, lr:2.000e-04> l_g_L1: 3.983e-02 l_g_distill: 2.574e-02 l_g_feature: 3.269e-03 G_loss: 6.883e-02 
25-12-10 01:04:04.113 : <epoch:121, iter:   3,400, lr:2.000e-04> l_g_L1: 4.242e-02 l_g_distill: 2.937e-02 l_g_feature: 3.025e-03 G_loss: 7.481e-02 
25-12-10 01:14:07.495 : <epoch:128, iter:   3,600, lr:2.000e-04> l_g_L1: 3.725e-02 l_g_distill: 2.059e-02 l_g_feature: 2.969e-03 G_loss: 6.081e-02 
25-12-10 01:24:10.716 : <epoch:135, iter:   3,800, lr:2.000e-04> l_g_L1: 3.290e-02 l_g_distill: 1.694e-02 l_g_feature: 2.523e-03 G_loss: 5.237e-02 
25-12-10 01:34:13.909 : <epoch:142, iter:   4,000, lr:2.000e-04> l_g_L1: 3.319e-02 l_g_distill: 1.637e-02 l_g_feature: 2.563e-03 G_loss: 5.212e-02 
25-12-10 01:34:13.909 : Saving the model.
25-12-10 01:34:16.893 : ---1--> babyx4.png | 31.39dB
25-12-10 01:34:16.951 : ---2--> birdx4.png | 29.25dB
25-12-10 01:34:16.989 : ---3--> butterflyx4.png | 22.17dB
25-12-10 01:34:17.045 : ---4--> headx4.png | 29.07dB
25-12-10 01:34:17.099 : ---5--> womanx4.png | 26.39dB
25-12-10 01:34:17.443 : <epoch:142, iter:   4,000, Average PSNR : 27.65dB

25-12-10 01:44:21.850 : <epoch:149, iter:   4,200, lr:2.000e-04> l_g_L1: 2.612e-02 l_g_distill: 1.510e-02 l_g_feature: 2.160e-03 G_loss: 4.338e-02 
25-12-10 01:54:25.314 : <epoch:157, iter:   4,400, lr:2.000e-04> l_g_L1: 2.783e-02 l_g_distill: 1.642e-02 l_g_feature: 2.439e-03 G_loss: 4.668e-02 
25-12-10 02:04:28.937 : <epoch:164, iter:   4,600, lr:2.000e-04> l_g_L1: 2.846e-02 l_g_distill: 1.699e-02 l_g_feature: 2.106e-03 G_loss: 4.756e-02 
25-12-10 02:14:31.571 : <epoch:171, iter:   4,800, lr:2.000e-04> l_g_L1: 2.155e-02 l_g_distill: 1.421e-02 l_g_feature: 1.971e-03 G_loss: 3.773e-02 
25-12-10 02:24:35.517 : <epoch:178, iter:   5,000, lr:2.000e-04> l_g_L1: 3.453e-02 l_g_distill: 2.116e-02 l_g_feature: 2.172e-03 G_loss: 5.787e-02 
25-12-10 02:34:39.148 : <epoch:185, iter:   5,200, lr:2.000e-04> l_g_L1: 3.242e-02 l_g_distill: 1.968e-02 l_g_feature: 2.129e-03 G_loss: 5.423e-02 
25-12-10 02:44:42.739 : <epoch:192, iter:   5,400, lr:2.000e-04> l_g_L1: 1.970e-02 l_g_distill: 1.159e-02 l_g_feature: 1.685e-03 G_loss: 3.297e-02 
25-12-10 02:54:46.732 : <epoch:199, iter:   5,600, lr:2.000e-04> l_g_L1: 3.752e-02 l_g_distill: 2.206e-02 l_g_feature: 2.166e-03 G_loss: 6.175e-02 
25-12-10 03:04:50.959 : <epoch:207, iter:   5,800, lr:2.000e-04> l_g_L1: 2.679e-02 l_g_distill: 1.494e-02 l_g_feature: 1.849e-03 G_loss: 4.358e-02 
25-12-10 03:14:55.190 : <epoch:214, iter:   6,000, lr:2.000e-04> l_g_L1: 2.247e-02 l_g_distill: 1.277e-02 l_g_feature: 1.674e-03 G_loss: 3.691e-02 
25-12-10 03:14:55.190 : Saving the model.
25-12-10 03:14:58.558 : ---1--> babyx4.png | 31.65dB
25-12-10 03:14:58.631 : ---2--> birdx4.png | 29.74dB
25-12-10 03:14:58.679 : ---3--> butterflyx4.png | 22.79dB
25-12-10 03:14:58.737 : ---4--> headx4.png | 29.19dB
25-12-10 03:14:58.791 : ---5--> womanx4.png | 27.05dB
25-12-10 03:14:59.125 : <epoch:214, iter:   6,000, Average PSNR : 28.09dB

25-12-10 03:25:06.973 : <epoch:221, iter:   6,200, lr:2.000e-04> l_g_L1: 2.479e-02 l_g_distill: 1.271e-02 l_g_feature: 1.620e-03 G_loss: 3.912e-02 
25-12-10 03:35:13.778 : <epoch:228, iter:   6,400, lr:2.000e-04> l_g_L1: 3.219e-02 l_g_distill: 2.121e-02 l_g_feature: 1.979e-03 G_loss: 5.538e-02 
25-12-10 03:45:18.562 : <epoch:235, iter:   6,600, lr:2.000e-04> l_g_L1: 3.391e-02 l_g_distill: 1.699e-02 l_g_feature: 1.657e-03 G_loss: 5.256e-02 
25-12-10 03:55:23.975 : <epoch:242, iter:   6,800, lr:2.000e-04> l_g_L1: 2.515e-02 l_g_distill: 1.368e-02 l_g_feature: 1.612e-03 G_loss: 4.044e-02 
25-12-10 04:05:28.603 : <epoch:249, iter:   7,000, lr:2.000e-04> l_g_L1: 2.675e-02 l_g_distill: 1.343e-02 l_g_feature: 1.495e-03 G_loss: 4.168e-02 
25-12-10 04:15:33.352 : <epoch:257, iter:   7,200, lr:2.000e-04> l_g_L1: 2.842e-02 l_g_distill: 1.514e-02 l_g_feature: 1.834e-03 G_loss: 4.540e-02 
25-12-10 04:25:38.216 : <epoch:264, iter:   7,400, lr:2.000e-04> l_g_L1: 2.719e-02 l_g_distill: 1.406e-02 l_g_feature: 1.605e-03 G_loss: 4.285e-02 
25-12-10 04:35:42.603 : <epoch:271, iter:   7,600, lr:2.000e-04> l_g_L1: 2.298e-02 l_g_distill: 1.359e-02 l_g_feature: 1.449e-03 G_loss: 3.802e-02 
25-12-10 04:45:46.391 : <epoch:278, iter:   7,800, lr:2.000e-04> l_g_L1: 3.193e-02 l_g_distill: 1.825e-02 l_g_feature: 1.687e-03 G_loss: 5.187e-02 
25-12-10 04:55:50.817 : <epoch:285, iter:   8,000, lr:2.000e-04> l_g_L1: 2.077e-02 l_g_distill: 9.275e-03 l_g_feature: 1.290e-03 G_loss: 3.133e-02 
25-12-10 04:55:50.817 : Saving the model.
25-12-10 04:55:53.784 : ---1--> babyx4.png | 31.66dB
25-12-10 04:55:53.845 : ---2--> birdx4.png | 29.96dB
25-12-10 04:55:53.883 : ---3--> butterflyx4.png | 23.44dB
25-12-10 04:55:53.940 : ---4--> headx4.png | 29.47dB
25-12-10 04:55:53.993 : ---5--> womanx4.png | 27.62dB
25-12-10 04:55:54.340 : <epoch:285, iter:   8,000, Average PSNR : 28.43dB

25-12-10 05:05:59.407 : <epoch:292, iter:   8,200, lr:2.000e-04> l_g_L1: 2.905e-02 l_g_distill: 1.493e-02 l_g_feature: 1.478e-03 G_loss: 4.546e-02 
25-12-10 05:16:03.347 : <epoch:299, iter:   8,400, lr:2.000e-04> l_g_L1: 3.742e-02 l_g_distill: 2.162e-02 l_g_feature: 1.518e-03 G_loss: 6.056e-02 
25-12-10 05:26:07.633 : <epoch:307, iter:   8,600, lr:2.000e-04> l_g_L1: 2.733e-02 l_g_distill: 1.205e-02 l_g_feature: 1.373e-03 G_loss: 4.076e-02 
25-12-10 05:36:12.001 : <epoch:314, iter:   8,800, lr:2.000e-04> l_g_L1: 3.397e-02 l_g_distill: 1.993e-02 l_g_feature: 1.369e-03 G_loss: 5.527e-02 
25-12-10 05:46:16.213 : <epoch:321, iter:   9,000, lr:2.000e-04> l_g_L1: 2.952e-02 l_g_distill: 1.402e-02 l_g_feature: 1.318e-03 G_loss: 4.486e-02 
25-12-10 05:56:20.389 : <epoch:328, iter:   9,200, lr:2.000e-04> l_g_L1: 3.298e-02 l_g_distill: 1.567e-02 l_g_feature: 1.410e-03 G_loss: 5.006e-02 
25-12-10 06:06:27.640 : <epoch:335, iter:   9,400, lr:2.000e-04> l_g_L1: 2.460e-02 l_g_distill: 1.189e-02 l_g_feature: 1.331e-03 G_loss: 3.782e-02 
25-12-10 06:16:31.996 : <epoch:342, iter:   9,600, lr:2.000e-04> l_g_L1: 2.557e-02 l_g_distill: 1.314e-02 l_g_feature: 1.370e-03 G_loss: 4.008e-02 
25-12-10 06:26:36.247 : <epoch:349, iter:   9,800, lr:2.000e-04> l_g_L1: 2.484e-02 l_g_distill: 1.299e-02 l_g_feature: 1.310e-03 G_loss: 3.914e-02 
25-12-10 06:36:41.634 : <epoch:357, iter:  10,000, lr:5.000e-05> l_g_L1: 3.002e-02 l_g_distill: 1.630e-02 l_g_feature: 1.302e-03 G_loss: 4.762e-02 
25-12-10 06:36:41.636 : Saving the model.
25-12-10 06:36:44.585 : ---1--> babyx4.png | 31.62dB
25-12-10 06:36:44.640 : ---2--> birdx4.png | 30.27dB
25-12-10 06:36:44.680 : ---3--> butterflyx4.png | 23.86dB
25-12-10 06:36:44.753 : ---4--> headx4.png | 29.42dB
25-12-10 06:36:44.802 : ---5--> womanx4.png | 27.75dB
25-12-10 06:36:45.150 : <epoch:357, iter:  10,000, Average PSNR : 28.58dB

25-12-10 06:46:49.627 : <epoch:364, iter:  10,200, lr:1.000e-04> l_g_L1: 2.917e-02 l_g_distill: 1.321e-02 l_g_feature: 1.092e-03 G_loss: 4.347e-02 
25-12-10 06:56:54.060 : <epoch:371, iter:  10,400, lr:1.000e-04> l_g_L1: 2.360e-02 l_g_distill: 1.325e-02 l_g_feature: 1.086e-03 G_loss: 3.794e-02 
25-12-10 07:06:58.243 : <epoch:378, iter:  10,600, lr:1.000e-04> l_g_L1: 2.620e-02 l_g_distill: 1.139e-02 l_g_feature: 9.891e-04 G_loss: 3.857e-02 
25-12-10 07:17:02.429 : <epoch:385, iter:  10,800, lr:1.000e-04> l_g_L1: 2.454e-02 l_g_distill: 9.997e-03 l_g_feature: 9.970e-04 G_loss: 3.553e-02 
25-12-10 07:27:06.891 : <epoch:392, iter:  11,000, lr:1.000e-04> l_g_L1: 2.583e-02 l_g_distill: 1.358e-02 l_g_feature: 1.061e-03 G_loss: 4.047e-02 
25-12-10 07:37:12.222 : <epoch:399, iter:  11,200, lr:1.000e-04> l_g_L1: 2.414e-02 l_g_distill: 1.208e-02 l_g_feature: 1.005e-03 G_loss: 3.723e-02 
25-12-10 07:47:17.420 : <epoch:407, iter:  11,400, lr:1.000e-04> l_g_L1: 2.726e-02 l_g_distill: 1.153e-02 l_g_feature: 9.578e-04 G_loss: 3.975e-02 
25-12-10 07:57:21.867 : <epoch:414, iter:  11,600, lr:1.000e-04> l_g_L1: 2.796e-02 l_g_distill: 1.292e-02 l_g_feature: 1.015e-03 G_loss: 4.190e-02 
25-12-10 08:07:26.469 : <epoch:421, iter:  11,800, lr:1.000e-04> l_g_L1: 4.041e-02 l_g_distill: 1.640e-02 l_g_feature: 1.073e-03 G_loss: 5.789e-02 
25-12-10 08:17:30.660 : <epoch:428, iter:  12,000, lr:1.000e-04> l_g_L1: 2.464e-02 l_g_distill: 1.023e-02 l_g_feature: 9.529e-04 G_loss: 3.582e-02 
25-12-10 08:17:30.660 : Saving the model.
25-12-10 08:17:33.894 : ---1--> babyx4.png | 31.82dB
25-12-10 08:17:33.965 : ---2--> birdx4.png | 30.49dB
25-12-10 08:17:34.013 : ---3--> butterflyx4.png | 24.03dB
25-12-10 08:17:34.071 : ---4--> headx4.png | 29.61dB
25-12-10 08:17:34.122 : ---5--> womanx4.png | 28.01dB
25-12-10 08:17:34.477 : <epoch:428, iter:  12,000, Average PSNR : 28.79dB

25-12-10 08:27:39.309 : <epoch:435, iter:  12,200, lr:1.000e-04> l_g_L1: 1.973e-02 l_g_distill: 9.182e-03 l_g_feature: 9.187e-04 G_loss: 2.983e-02 
25-12-10 08:37:44.297 : <epoch:442, iter:  12,400, lr:1.000e-04> l_g_L1: 2.678e-02 l_g_distill: 1.228e-02 l_g_feature: 9.427e-04 G_loss: 4.000e-02 
25-12-10 08:47:48.895 : <epoch:449, iter:  12,600, lr:1.000e-04> l_g_L1: 3.754e-02 l_g_distill: 1.556e-02 l_g_feature: 9.822e-04 G_loss: 5.408e-02 
25-12-10 08:57:54.500 : <epoch:457, iter:  12,800, lr:1.000e-04> l_g_L1: 2.753e-02 l_g_distill: 1.318e-02 l_g_feature: 9.226e-04 G_loss: 4.163e-02 
25-12-10 09:07:58.304 : <epoch:464, iter:  13,000, lr:1.000e-04> l_g_L1: 2.135e-02 l_g_distill: 1.127e-02 l_g_feature: 8.521e-04 G_loss: 3.348e-02 
25-12-10 09:18:05.302 : <epoch:471, iter:  13,200, lr:1.000e-04> l_g_L1: 2.599e-02 l_g_distill: 1.347e-02 l_g_feature: 9.306e-04 G_loss: 4.039e-02 
25-12-10 09:31:18.325 : <epoch:478, iter:  13,400, lr:1.000e-04> l_g_L1: 2.568e-02 l_g_distill: 1.069e-02 l_g_feature: 8.997e-04 G_loss: 3.727e-02 
25-12-10 09:44:29.795 : <epoch:485, iter:  13,600, lr:1.000e-04> l_g_L1: 2.813e-02 l_g_distill: 1.347e-02 l_g_feature: 8.933e-04 G_loss: 4.249e-02 
25-12-10 09:58:13.372 : <epoch:492, iter:  13,800, lr:1.000e-04> l_g_L1: 2.718e-02 l_g_distill: 1.131e-02 l_g_feature: 9.055e-04 G_loss: 3.940e-02 
25-12-10 10:12:07.308 : <epoch:499, iter:  14,000, lr:1.000e-04> l_g_L1: 3.144e-02 l_g_distill: 1.167e-02 l_g_feature: 8.894e-04 G_loss: 4.400e-02 
25-12-10 10:12:07.309 : Saving the model.
25-12-10 10:12:11.530 : ---1--> babyx4.png | 31.93dB
25-12-10 10:12:11.615 : ---2--> birdx4.png | 30.76dB
25-12-10 10:12:11.658 : ---3--> butterflyx4.png | 24.30dB
25-12-10 10:12:11.743 : ---4--> headx4.png | 29.65dB
25-12-10 10:12:11.824 : ---5--> womanx4.png | 28.06dB
25-12-10 10:12:12.284 : <epoch:499, iter:  14,000, Average PSNR : 28.94dB

25-12-10 10:26:16.435 : <epoch:507, iter:  14,200, lr:1.000e-04> l_g_L1: 2.731e-02 l_g_distill: 1.397e-02 l_g_feature: 9.083e-04 G_loss: 4.219e-02 
25-12-10 10:39:50.929 : <epoch:514, iter:  14,400, lr:1.000e-04> l_g_L1: 2.033e-02 l_g_distill: 8.019e-03 l_g_feature: 8.400e-04 G_loss: 2.919e-02 
25-12-10 10:50:35.707 : <epoch:521, iter:  14,600, lr:1.000e-04> l_g_L1: 2.563e-02 l_g_distill: 1.188e-02 l_g_feature: 8.737e-04 G_loss: 3.838e-02 
25-12-10 11:00:56.311 : <epoch:528, iter:  14,800, lr:1.000e-04> l_g_L1: 2.034e-02 l_g_distill: 1.037e-02 l_g_feature: 8.578e-04 G_loss: 3.157e-02 
25-12-10 11:11:12.335 : <epoch:535, iter:  15,000, lr:1.000e-04> l_g_L1: 2.375e-02 l_g_distill: 1.133e-02 l_g_feature: 9.141e-04 G_loss: 3.600e-02 
25-12-10 11:21:25.719 : <epoch:542, iter:  15,200, lr:1.000e-04> l_g_L1: 2.380e-02 l_g_distill: 1.048e-02 l_g_feature: 8.287e-04 G_loss: 3.510e-02 
25-12-10 11:31:42.315 : <epoch:549, iter:  15,400, lr:1.000e-04> l_g_L1: 3.231e-02 l_g_distill: 1.340e-02 l_g_feature: 8.843e-04 G_loss: 4.660e-02 
25-12-10 11:42:29.128 : <epoch:557, iter:  15,600, lr:1.000e-04> l_g_L1: 2.805e-02 l_g_distill: 1.261e-02 l_g_feature: 8.204e-04 G_loss: 4.148e-02 
25-12-10 11:54:18.489 : <epoch:564, iter:  15,800, lr:1.000e-04> l_g_L1: 1.962e-02 l_g_distill: 9.601e-03 l_g_feature: 8.056e-04 G_loss: 3.002e-02 
25-12-10 12:05:55.020 : <epoch:571, iter:  16,000, lr:2.500e-05> l_g_L1: 2.691e-02 l_g_distill: 1.020e-02 l_g_feature: 7.776e-04 G_loss: 3.789e-02 
25-12-10 12:05:55.020 : Saving the model.
25-12-10 12:05:59.016 : ---1--> babyx4.png | 31.98dB
25-12-10 12:05:59.071 : ---2--> birdx4.png | 30.85dB
25-12-10 12:05:59.106 : ---3--> butterflyx4.png | 24.47dB
25-12-10 12:05:59.156 : ---4--> headx4.png | 29.68dB
25-12-10 12:05:59.210 : ---5--> womanx4.png | 28.25dB
25-12-10 12:05:59.627 : <epoch:571, iter:  16,000, Average PSNR : 29.05dB

25-12-10 12:16:19.042 : <epoch:578, iter:  16,200, lr:5.000e-05> l_g_L1: 3.084e-02 l_g_distill: 1.452e-02 l_g_feature: 7.716e-04 G_loss: 4.613e-02 
25-12-10 12:26:33.442 : <epoch:585, iter:  16,400, lr:5.000e-05> l_g_L1: 2.859e-02 l_g_distill: 1.292e-02 l_g_feature: 7.686e-04 G_loss: 4.228e-02 
25-12-10 12:36:44.836 : <epoch:592, iter:  16,600, lr:5.000e-05> l_g_L1: 2.816e-02 l_g_distill: 1.269e-02 l_g_feature: 7.407e-04 G_loss: 4.159e-02 
25-12-10 12:46:55.250 : <epoch:599, iter:  16,800, lr:5.000e-05> l_g_L1: 2.318e-02 l_g_distill: 1.115e-02 l_g_feature: 7.483e-04 G_loss: 3.508e-02 
25-12-10 12:57:04.857 : <epoch:607, iter:  17,000, lr:5.000e-05> l_g_L1: 2.921e-02 l_g_distill: 1.265e-02 l_g_feature: 7.944e-04 G_loss: 4.266e-02 
25-12-10 13:07:15.212 : <epoch:614, iter:  17,200, lr:5.000e-05> l_g_L1: 2.593e-02 l_g_distill: 1.135e-02 l_g_feature: 7.248e-04 G_loss: 3.800e-02 
25-12-10 13:17:25.397 : <epoch:621, iter:  17,400, lr:5.000e-05> l_g_L1: 2.576e-02 l_g_distill: 1.109e-02 l_g_feature: 7.470e-04 G_loss: 3.759e-02 
25-12-10 13:27:35.681 : <epoch:628, iter:  17,600, lr:5.000e-05> l_g_L1: 3.393e-02 l_g_distill: 1.491e-02 l_g_feature: 7.484e-04 G_loss: 4.959e-02 
25-12-10 13:37:46.672 : <epoch:635, iter:  17,800, lr:5.000e-05> l_g_L1: 2.439e-02 l_g_distill: 9.946e-03 l_g_feature: 7.028e-04 G_loss: 3.504e-02 
25-12-10 13:48:06.628 : <epoch:642, iter:  18,000, lr:1.250e-05> l_g_L1: 2.647e-02 l_g_distill: 1.315e-02 l_g_feature: 7.199e-04 G_loss: 4.034e-02 
25-12-10 13:48:06.628 : Saving the model.
25-12-10 13:48:09.859 : ---1--> babyx4.png | 32.06dB
25-12-10 13:48:09.927 : ---2--> birdx4.png | 30.96dB
25-12-10 13:48:09.977 : ---3--> butterflyx4.png | 24.60dB
25-12-10 13:48:10.039 : ---4--> headx4.png | 29.71dB
25-12-10 13:48:10.092 : ---5--> womanx4.png | 28.31dB
25-12-10 13:48:10.443 : <epoch:642, iter:  18,000, Average PSNR : 29.13dB

25-12-10 13:58:19.470 : <epoch:649, iter:  18,200, lr:2.500e-05> l_g_L1: 3.178e-02 l_g_distill: 1.445e-02 l_g_feature: 7.307e-04 G_loss: 4.695e-02 
25-12-10 14:08:27.855 : <epoch:657, iter:  18,400, lr:2.500e-05> l_g_L1: 2.966e-02 l_g_distill: 1.349e-02 l_g_feature: 7.207e-04 G_loss: 4.386e-02 
25-12-10 14:18:36.056 : <epoch:664, iter:  18,600, lr:2.500e-05> l_g_L1: 2.534e-02 l_g_distill: 1.166e-02 l_g_feature: 6.203e-04 G_loss: 3.761e-02 
25-12-10 14:28:45.281 : <epoch:671, iter:  18,800, lr:2.500e-05> l_g_L1: 2.968e-02 l_g_distill: 1.511e-02 l_g_feature: 7.624e-04 G_loss: 4.556e-02 
25-12-10 14:38:53.682 : <epoch:678, iter:  19,000, lr:6.250e-06> l_g_L1: 2.907e-02 l_g_distill: 1.355e-02 l_g_feature: 7.245e-04 G_loss: 4.335e-02 
25-12-10 14:49:02.272 : <epoch:685, iter:  19,200, lr:1.250e-05> l_g_L1: 2.840e-02 l_g_distill: 1.256e-02 l_g_feature: 6.984e-04 G_loss: 4.165e-02 
25-12-10 14:59:11.088 : <epoch:692, iter:  19,400, lr:1.250e-05> l_g_L1: 2.521e-02 l_g_distill: 1.006e-02 l_g_feature: 5.923e-04 G_loss: 3.586e-02 
25-12-10 15:09:21.899 : <epoch:699, iter:  19,600, lr:1.250e-05> l_g_L1: 2.397e-02 l_g_distill: 9.518e-03 l_g_feature: 6.274e-04 G_loss: 3.411e-02 
25-12-10 15:19:30.424 : <epoch:707, iter:  19,800, lr:1.250e-05> l_g_L1: 3.240e-02 l_g_distill: 1.380e-02 l_g_feature: 7.099e-04 G_loss: 4.691e-02 
25-12-10 15:29:37.829 : <epoch:714, iter:  20,000, lr:1.250e-05> l_g_L1: 2.917e-02 l_g_distill: 1.536e-02 l_g_feature: 6.931e-04 G_loss: 4.522e-02 
25-12-10 15:29:37.829 : Saving the model.
25-12-10 15:29:41.096 : ---1--> babyx4.png | 32.04dB
25-12-10 15:29:41.167 : ---2--> birdx4.png | 31.03dB
25-12-10 15:29:41.216 : ---3--> butterflyx4.png | 24.70dB
25-12-10 15:29:41.272 : ---4--> headx4.png | 29.73dB
25-12-10 15:29:41.326 : ---5--> womanx4.png | 28.35dB
25-12-10 15:29:41.672 : <epoch:714, iter:  20,000, Average PSNR : 29.17dB

25-12-10 15:39:48.717 : <epoch:721, iter:  20,200, lr:1.250e-05> l_g_L1: 2.209e-02 l_g_distill: 8.840e-03 l_g_feature: 6.254e-04 G_loss: 3.156e-02 
25-12-10 15:49:54.715 : <epoch:728, iter:  20,400, lr:1.250e-05> l_g_L1: 2.688e-02 l_g_distill: 1.080e-02 l_g_feature: 6.519e-04 G_loss: 3.832e-02 
25-12-10 15:59:59.682 : <epoch:735, iter:  20,600, lr:1.250e-05> l_g_L1: 2.453e-02 l_g_distill: 1.168e-02 l_g_feature: 6.047e-04 G_loss: 3.682e-02 
25-12-10 16:10:05.108 : <epoch:742, iter:  20,800, lr:1.250e-05> l_g_L1: 2.502e-02 l_g_distill: 1.313e-02 l_g_feature: 6.154e-04 G_loss: 3.876e-02 
25-12-10 16:20:11.285 : <epoch:749, iter:  21,000, lr:1.250e-05> l_g_L1: 2.490e-02 l_g_distill: 1.142e-02 l_g_feature: 6.375e-04 G_loss: 3.696e-02 
25-12-10 16:30:15.886 : <epoch:757, iter:  21,200, lr:1.250e-05> l_g_L1: 1.820e-02 l_g_distill: 8.625e-03 l_g_feature: 5.633e-04 G_loss: 2.739e-02 
25-12-10 16:40:22.113 : <epoch:764, iter:  21,400, lr:1.250e-05> l_g_L1: 1.766e-02 l_g_distill: 6.614e-03 l_g_feature: 5.139e-04 G_loss: 2.479e-02 
25-12-10 16:50:29.334 : <epoch:771, iter:  21,600, lr:1.250e-05> l_g_L1: 3.231e-02 l_g_distill: 1.434e-02 l_g_feature: 6.395e-04 G_loss: 4.729e-02 
25-12-10 17:00:38.749 : <epoch:778, iter:  21,800, lr:1.250e-05> l_g_L1: 2.171e-02 l_g_distill: 8.967e-03 l_g_feature: 5.696e-04 G_loss: 3.125e-02 
25-12-10 17:10:49.905 : <epoch:785, iter:  22,000, lr:1.250e-05> l_g_L1: 2.512e-02 l_g_distill: 1.224e-02 l_g_feature: 6.501e-04 G_loss: 3.801e-02 
25-12-10 17:10:49.905 : Saving the model.
25-12-10 17:10:53.048 : ---1--> babyx4.png | 32.03dB
25-12-10 17:10:53.109 : ---2--> birdx4.png | 31.06dB
25-12-10 17:10:53.148 : ---3--> butterflyx4.png | 24.73dB
25-12-10 17:10:53.198 : ---4--> headx4.png | 29.73dB
25-12-10 17:10:53.250 : ---5--> womanx4.png | 28.37dB
25-12-10 17:10:53.600 : <epoch:785, iter:  22,000, Average PSNR : 29.18dB

25-12-10 17:21:02.942 : <epoch:792, iter:  22,200, lr:1.250e-05> l_g_L1: 2.408e-02 l_g_distill: 8.448e-03 l_g_feature: 5.246e-04 G_loss: 3.305e-02 
25-12-10 17:31:11.716 : <epoch:799, iter:  22,400, lr:1.250e-05> l_g_L1: 2.542e-02 l_g_distill: 1.072e-02 l_g_feature: 6.435e-04 G_loss: 3.679e-02 
25-12-10 17:41:21.880 : <epoch:807, iter:  22,600, lr:1.250e-05> l_g_L1: 2.611e-02 l_g_distill: 1.326e-02 l_g_feature: 6.133e-04 G_loss: 3.999e-02 
25-12-10 17:51:31.095 : <epoch:814, iter:  22,800, lr:1.250e-05> l_g_L1: 3.311e-02 l_g_distill: 1.166e-02 l_g_feature: 6.428e-04 G_loss: 4.542e-02 
25-12-10 18:01:39.947 : <epoch:821, iter:  23,000, lr:1.250e-05> l_g_L1: 2.259e-02 l_g_distill: 8.892e-03 l_g_feature: 5.535e-04 G_loss: 3.203e-02 
25-12-10 18:12:11.929 : <epoch:828, iter:  23,200, lr:1.250e-05> l_g_L1: 1.932e-02 l_g_distill: 9.880e-03 l_g_feature: 5.664e-04 G_loss: 2.977e-02 
25-12-10 18:22:15.915 : <epoch:835, iter:  23,400, lr:1.250e-05> l_g_L1: 2.545e-02 l_g_distill: 1.150e-02 l_g_feature: 6.360e-04 G_loss: 3.758e-02 
25-12-10 18:32:22.936 : <epoch:842, iter:  23,600, lr:1.250e-05> l_g_L1: 2.598e-02 l_g_distill: 1.471e-02 l_g_feature: 5.755e-04 G_loss: 4.127e-02 
