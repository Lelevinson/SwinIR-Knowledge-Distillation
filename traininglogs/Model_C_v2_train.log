25-12-04 12:41:56.555 :   task: student_C_v2_delayed_x4
  model: plain
  gpu_ids: [0]
  dist: False
  scale: 4
  n_channels: 3
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netE: None
    pretrained_netTeacher: model_zoo/swinir/001_classicalSR_DF2K_s64w8_SwinIR-M_x4.pth
    task: superresolution\student_C_v2_delayed_x4
    log: superresolution\student_C_v2_delayed_x4
    options: superresolution\student_C_v2_delayed_x4\options
    models: superresolution\student_C_v2_delayed_x4\models
    images: superresolution\student_C_v2_delayed_x4\images
    pretrained_netP: None
    pretrained_optimizerG: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/trainH/DIV2K/DIV2K_train_HR
      dataroot_L: trainsets/trainH/DIV2K/DIV2K_train_LR_bicubic/X4
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 0
      dataloader_batch_size: 32
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/Set5/HR
      dataroot_L: testsets/Set5/LR_bicubic/X4
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: swinir_student_v2
    upscale: 4
    in_chans: 3
    img_size: 64
    window_size: 8
    img_range: 1.0
    depths: [4, 4, 4, 4]
    embed_dim: 60
    num_heads: [6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 4
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    distillation_type: feature
    distill_lossfn_type: l1
    distill_lossfn_weight: 1.0
    feature_lossfn_type: l1
    feature_lossfn_weight: 0.01
    start_feature_iter: 4000
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [10000, 16000, 18000, 19000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_save: 2000
    checkpoint_test: 2000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/swinir/train_swinir_student_distill_feature_v2.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: False
  use_static_graph: False
  num_gpu: 1
  rank: 0
  world_size: 1

25-12-04 12:41:56.567 : Number of train images: 900, iters: 29
25-12-04 12:41:58.642 : 
Networks name: SwinIR_Student_v2
Params number: 988959
Net structure:
SwinIR_Student_v2(
  (conv_first): Conv2d(3, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=60, input_resolution=(64, 64), depth=4
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.007)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.013)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.020)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=60, input_resolution=(64, 64), depth=4
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.027)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.033)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.040)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.047)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=60, input_resolution=(64, 64), depth=4
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.053)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.060)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.067)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.073)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=60, input_resolution=(64, 64), depth=4
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.080)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.087)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.093)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=60, input_resolution=(64, 64), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=60, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=60, out_features=180, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=60, out_features=60, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.100)
            (norm2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=60, out_features=120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=120, out_features=60, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((60,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
    (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

25-12-04 12:41:58.718 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.002 | -0.192 |  0.192 |  0.111 | torch.Size([60, 3, 3, 3]) || conv_first.weight
 |  0.008 | -0.183 |  0.190 |  0.106 | torch.Size([60]) || conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.066 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.087 |  0.075 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.001 | -0.073 |  0.063 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.086 |  0.072 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.075 |  0.091 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.077 |  0.072 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.076 |  0.082 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.074 |  0.070 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.071 |  0.075 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.073 |  0.087 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.054 |  0.061 |  0.019 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.079 |  0.071 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.069 |  0.063 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.074 |  0.073 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.073 |  0.077 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.067 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.074 |  0.070 |  0.020 | torch.Size([180, 60]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.001 | -0.076 |  0.070 |  0.020 | torch.Size([60, 60]) || layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.078 |  0.069 |  0.020 | torch.Size([120, 60]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.068 |  0.075 |  0.020 | torch.Size([60, 120]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([60, 60, 3, 3]) || layers.0.conv.weight
 | -0.004 | -0.043 |  0.043 |  0.021 | torch.Size([60]) || layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.073 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.091 |  0.073 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.065 |  0.064 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.080 |  0.079 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.076 |  0.081 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.073 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.073 |  0.070 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.076 |  0.069 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.084 |  0.072 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.067 |  0.076 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.060 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.068 |  0.079 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.001 | -0.059 |  0.059 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.071 |  0.092 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.076 |  0.082 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.068 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.076 |  0.089 |  0.020 | torch.Size([180, 60]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.066 |  0.076 |  0.020 | torch.Size([60, 60]) || layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.071 |  0.087 |  0.020 | torch.Size([120, 60]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.075 |  0.079 |  0.020 | torch.Size([60, 120]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([60, 60, 3, 3]) || layers.1.conv.weight
 | -0.002 | -0.043 |  0.042 |  0.024 | torch.Size([60]) || layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.068 |  0.072 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.080 |  0.081 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.068 |  0.071 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.071 |  0.079 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.070 |  0.077 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.072 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.079 |  0.076 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.073 |  0.073 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.102 |  0.069 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.077 |  0.083 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.058 |  0.072 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.084 |  0.071 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.077 |  0.068 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.078 |  0.074 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.076 |  0.069 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.081 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.076 |  0.077 |  0.020 | torch.Size([180, 60]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.088 |  0.072 |  0.020 | torch.Size([60, 60]) || layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.072 |  0.073 |  0.020 | torch.Size([120, 60]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.076 |  0.070 |  0.020 | torch.Size([60, 120]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([60, 60, 3, 3]) || layers.2.conv.weight
 | -0.000 | -0.043 |  0.040 |  0.025 | torch.Size([60]) || layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.062 |  0.056 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.065 |  0.077 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.062 |  0.068 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.074 |  0.073 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.082 |  0.080 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.061 |  0.056 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.083 |  0.071 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.080 |  0.066 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.081 |  0.068 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.070 |  0.075 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.074 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.075 |  0.087 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 |  0.001 | -0.076 |  0.079 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.064 |  0.072 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.079 |  0.075 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -12.109 | -100.000 |  0.000 | 32.624 | torch.Size([64, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.062 |  0.067 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.080 |  0.074 |  0.020 | torch.Size([180, 60]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.073 |  0.073 |  0.020 | torch.Size([60, 60]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.073 |  0.070 |  0.020 | torch.Size([120, 60]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([120]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.068 |  0.091 |  0.020 | torch.Size([60, 120]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([60, 60, 3, 3]) || layers.3.conv.weight
 | -0.001 | -0.043 |  0.042 |  0.027 | torch.Size([60]) || layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([60]) || norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([60]) || norm.bias
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([60, 60, 3, 3]) || conv_after_body.weight
 | -0.001 | -0.043 |  0.040 |  0.026 | torch.Size([60]) || conv_after_body.bias
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 60, 3, 3]) || conv_before_upsample.0.weight
 | -0.001 | -0.041 |  0.042 |  0.025 | torch.Size([64]) || conv_before_upsample.0.bias
 |  0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.0.weight
 | -0.002 | -0.042 |  0.041 |  0.025 | torch.Size([256]) || upsample.0.bias
 | -0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.2.weight
 |  0.002 | -0.042 |  0.042 |  0.024 | torch.Size([256]) || upsample.2.bias
 |  0.000 | -0.042 |  0.042 |  0.024 | torch.Size([3, 64, 3, 3]) || conv_last.weight
 | -0.005 | -0.027 |  0.021 |  0.024 | torch.Size([3]) || conv_last.bias

25-12-04 12:52:07.060 : <epoch:  7, iter:     200, lr:2.000e-04> l_g_L1: 7.539e-02 l_g_distill: 6.369e-02 l_g_feature: 2.395e+00 G_loss: 2.534e+00 
25-12-04 13:02:19.453 : <epoch: 14, iter:     400, lr:2.000e-04> l_g_L1: 4.519e-02 l_g_distill: 3.627e-02 l_g_feature: 8.709e-01 G_loss: 9.523e-01 
25-12-04 13:12:29.264 : <epoch: 21, iter:     600, lr:2.000e-04> l_g_L1: 3.999e-02 l_g_distill: 3.431e-02 l_g_feature: 5.795e-01 G_loss: 6.538e-01 
25-12-04 13:22:40.268 : <epoch: 28, iter:     800, lr:2.000e-04> l_g_L1: 3.623e-02 l_g_distill: 2.849e-02 l_g_feature: 3.356e-01 G_loss: 4.003e-01 
25-12-04 13:32:50.264 : <epoch: 35, iter:   1,000, lr:2.000e-04> l_g_L1: 3.670e-02 l_g_distill: 2.785e-02 l_g_feature: 2.402e-01 G_loss: 3.048e-01 
25-12-04 13:43:00.071 : <epoch: 42, iter:   1,200, lr:2.000e-04> l_g_L1: 3.368e-02 l_g_distill: 2.611e-02 l_g_feature: 1.971e-01 G_loss: 2.569e-01 
25-12-04 13:53:11.066 : <epoch: 49, iter:   1,400, lr:2.000e-04> l_g_L1: 3.838e-02 l_g_distill: 2.872e-02 l_g_feature: 1.473e-01 G_loss: 2.144e-01 
25-12-04 14:03:20.644 : <epoch: 57, iter:   1,600, lr:2.000e-04> l_g_L1: 2.968e-02 l_g_distill: 2.117e-02 l_g_feature: 8.303e-02 G_loss: 1.339e-01 
25-12-04 14:13:31.028 : <epoch: 64, iter:   1,800, lr:2.000e-04> l_g_L1: 3.264e-02 l_g_distill: 2.210e-02 l_g_feature: 6.975e-02 G_loss: 1.245e-01 
25-12-04 14:23:40.278 : <epoch: 71, iter:   2,000, lr:2.000e-04> l_g_L1: 3.345e-02 l_g_distill: 2.389e-02 l_g_feature: 4.714e-02 G_loss: 1.045e-01 
25-12-04 14:23:40.279 : Saving the model.
25-12-04 14:23:43.220 : ---1--> babyx4.png | 30.57dB
25-12-04 14:23:43.282 : ---2--> birdx4.png | 27.42dB
25-12-04 14:23:43.323 : ---3--> butterflyx4.png | 21.25dB
25-12-04 14:23:43.374 : ---4--> headx4.png | 28.80dB
25-12-04 14:23:43.431 : ---5--> womanx4.png | 25.69dB
25-12-04 14:23:43.779 : <epoch: 71, iter:   2,000, Average PSNR : 26.75dB

25-12-04 14:33:54.244 : <epoch: 78, iter:   2,200, lr:2.000e-04> l_g_L1: 3.753e-02 l_g_distill: 2.277e-02 l_g_feature: 4.061e-02 G_loss: 1.009e-01 
25-12-04 14:44:04.082 : <epoch: 85, iter:   2,400, lr:2.000e-04> l_g_L1: 3.676e-02 l_g_distill: 2.274e-02 l_g_feature: 3.170e-02 G_loss: 9.120e-02 
25-12-04 14:54:14.495 : <epoch: 92, iter:   2,600, lr:2.000e-04> l_g_L1: 3.201e-02 l_g_distill: 2.159e-02 l_g_feature: 2.703e-02 G_loss: 8.064e-02 
25-12-04 15:04:25.080 : <epoch: 99, iter:   2,800, lr:2.000e-04> l_g_L1: 4.004e-02 l_g_distill: 2.606e-02 l_g_feature: 2.295e-02 G_loss: 8.906e-02 
25-12-04 15:14:35.211 : <epoch:107, iter:   3,000, lr:2.000e-04> l_g_L1: 3.023e-02 l_g_distill: 1.892e-02 l_g_feature: 1.884e-02 G_loss: 6.799e-02 
25-12-04 15:24:44.663 : <epoch:114, iter:   3,200, lr:2.000e-04> l_g_L1: 2.344e-02 l_g_distill: 1.611e-02 l_g_feature: 1.428e-02 G_loss: 5.383e-02 
25-12-04 15:34:54.308 : <epoch:121, iter:   3,400, lr:2.000e-04> l_g_L1: 2.896e-02 l_g_distill: 1.678e-02 l_g_feature: 1.267e-02 G_loss: 5.842e-02 
25-12-04 15:45:05.292 : <epoch:128, iter:   3,600, lr:2.000e-04> l_g_L1: 2.345e-02 l_g_distill: 1.725e-02 l_g_feature: 1.039e-02 G_loss: 5.109e-02 
25-12-04 15:55:17.473 : <epoch:135, iter:   3,800, lr:2.000e-04> l_g_L1: 3.416e-02 l_g_distill: 2.227e-02 l_g_feature: 1.161e-02 G_loss: 6.804e-02 
25-12-04 16:05:27.882 : <epoch:142, iter:   4,000, lr:2.000e-04> l_g_L1: 4.130e-02 l_g_distill: 2.132e-02 l_g_feature: 1.175e-02 G_loss: 7.437e-02 
25-12-04 16:05:27.882 : Saving the model.
25-12-04 16:05:30.789 : ---1--> babyx4.png | 31.08dB
25-12-04 16:05:30.855 : ---2--> birdx4.png | 28.63dB
25-12-04 16:05:30.888 : ---3--> butterflyx4.png | 21.69dB
25-12-04 16:05:30.939 : ---4--> headx4.png | 28.99dB
25-12-04 16:05:30.990 : ---5--> womanx4.png | 26.01dB
25-12-04 16:05:31.341 : <epoch:142, iter:   4,000, Average PSNR : 27.28dB

25-12-04 16:15:42.277 : <epoch:149, iter:   4,200, lr:2.000e-04> l_g_L1: 3.368e-02 l_g_distill: 2.022e-02 l_g_feature: 8.897e-03 G_loss: 6.280e-02 
25-12-04 16:25:52.083 : <epoch:157, iter:   4,400, lr:2.000e-04> l_g_L1: 2.699e-02 l_g_distill: 1.691e-02 l_g_feature: 7.803e-03 G_loss: 5.171e-02 
25-12-04 16:36:01.697 : <epoch:164, iter:   4,600, lr:2.000e-04> l_g_L1: 2.776e-02 l_g_distill: 1.760e-02 l_g_feature: 7.304e-03 G_loss: 5.266e-02 
25-12-04 16:46:13.083 : <epoch:171, iter:   4,800, lr:2.000e-04> l_g_L1: 3.741e-02 l_g_distill: 2.032e-02 l_g_feature: 7.263e-03 G_loss: 6.499e-02 
25-12-04 16:56:24.088 : <epoch:178, iter:   5,000, lr:2.000e-04> l_g_L1: 3.768e-02 l_g_distill: 2.115e-02 l_g_feature: 6.808e-03 G_loss: 6.563e-02 
25-12-04 17:06:32.464 : <epoch:185, iter:   5,200, lr:2.000e-04> l_g_L1: 3.012e-02 l_g_distill: 1.768e-02 l_g_feature: 6.236e-03 G_loss: 5.403e-02 
25-12-04 17:16:44.519 : <epoch:192, iter:   5,400, lr:2.000e-04> l_g_L1: 3.643e-02 l_g_distill: 2.004e-02 l_g_feature: 6.789e-03 G_loss: 6.325e-02 
25-12-04 17:26:56.301 : <epoch:199, iter:   5,600, lr:2.000e-04> l_g_L1: 3.023e-02 l_g_distill: 1.681e-02 l_g_feature: 5.782e-03 G_loss: 5.282e-02 
25-12-04 17:40:07.474 : <epoch:207, iter:   5,800, lr:2.000e-04> l_g_L1: 2.951e-02 l_g_distill: 1.899e-02 l_g_feature: 5.957e-03 G_loss: 5.446e-02 
25-12-04 17:53:11.843 : <epoch:214, iter:   6,000, lr:2.000e-04> l_g_L1: 3.043e-02 l_g_distill: 1.782e-02 l_g_feature: 6.152e-03 G_loss: 5.441e-02 
25-12-04 17:53:11.844 : Saving the model.
25-12-04 17:53:16.006 : ---1--> babyx4.png | 31.29dB
25-12-04 17:53:16.082 : ---2--> birdx4.png | 29.34dB
25-12-04 17:53:16.136 : ---3--> butterflyx4.png | 22.00dB
25-12-04 17:53:16.205 : ---4--> headx4.png | 29.20dB
25-12-04 17:53:16.280 : ---5--> womanx4.png | 26.25dB
25-12-04 17:53:16.710 : <epoch:214, iter:   6,000, Average PSNR : 27.62dB

25-12-04 18:06:51.608 : <epoch:221, iter:   6,200, lr:2.000e-04> l_g_L1: 2.954e-02 l_g_distill: 1.753e-02 l_g_feature: 4.921e-03 G_loss: 5.200e-02 
25-12-04 18:20:13.349 : <epoch:228, iter:   6,400, lr:2.000e-04> l_g_L1: 2.875e-02 l_g_distill: 1.579e-02 l_g_feature: 5.508e-03 G_loss: 5.005e-02 
25-12-04 18:33:48.816 : <epoch:235, iter:   6,600, lr:2.000e-04> l_g_L1: 2.333e-02 l_g_distill: 1.417e-02 l_g_feature: 4.506e-03 G_loss: 4.200e-02 
25-12-04 18:46:07.605 : <epoch:242, iter:   6,800, lr:2.000e-04> l_g_L1: 2.666e-02 l_g_distill: 1.635e-02 l_g_feature: 4.413e-03 G_loss: 4.743e-02 
25-12-04 18:58:33.823 : <epoch:249, iter:   7,000, lr:2.000e-04> l_g_L1: 2.925e-02 l_g_distill: 1.578e-02 l_g_feature: 4.925e-03 G_loss: 4.996e-02 
25-12-04 19:09:31.247 : <epoch:257, iter:   7,200, lr:2.000e-04> l_g_L1: 4.118e-02 l_g_distill: 1.851e-02 l_g_feature: 3.898e-03 G_loss: 6.359e-02 
25-12-04 19:21:18.729 : <epoch:264, iter:   7,400, lr:2.000e-04> l_g_L1: 3.276e-02 l_g_distill: 1.573e-02 l_g_feature: 3.838e-03 G_loss: 5.233e-02 
25-12-04 19:34:38.253 : <epoch:271, iter:   7,600, lr:2.000e-04> l_g_L1: 2.618e-02 l_g_distill: 1.301e-02 l_g_feature: 3.989e-03 G_loss: 4.317e-02 
25-12-04 19:46:22.048 : <epoch:278, iter:   7,800, lr:2.000e-04> l_g_L1: 2.872e-02 l_g_distill: 1.595e-02 l_g_feature: 3.299e-03 G_loss: 4.797e-02 
25-12-04 19:59:54.936 : <epoch:285, iter:   8,000, lr:2.000e-04> l_g_L1: 2.870e-02 l_g_distill: 1.537e-02 l_g_feature: 3.990e-03 G_loss: 4.805e-02 
25-12-04 19:59:54.936 : Saving the model.
25-12-04 19:59:59.143 : ---1--> babyx4.png | 31.25dB
25-12-04 19:59:59.224 : ---2--> birdx4.png | 29.62dB
25-12-04 19:59:59.270 : ---3--> butterflyx4.png | 22.42dB
25-12-04 19:59:59.336 : ---4--> headx4.png | 29.21dB
25-12-04 19:59:59.409 : ---5--> womanx4.png | 26.46dB
25-12-04 19:59:59.826 : <epoch:285, iter:   8,000, Average PSNR : 27.79dB

25-12-04 20:13:41.923 : <epoch:292, iter:   8,200, lr:2.000e-04> l_g_L1: 2.505e-02 l_g_distill: 1.581e-02 l_g_feature: 3.987e-03 G_loss: 4.485e-02 
25-12-04 20:27:23.959 : <epoch:299, iter:   8,400, lr:2.000e-04> l_g_L1: 3.316e-02 l_g_distill: 1.749e-02 l_g_feature: 3.620e-03 G_loss: 5.426e-02 
25-12-04 20:41:02.935 : <epoch:307, iter:   8,600, lr:2.000e-04> l_g_L1: 3.111e-02 l_g_distill: 1.950e-02 l_g_feature: 3.494e-03 G_loss: 5.411e-02 
25-12-04 20:54:50.119 : <epoch:314, iter:   8,800, lr:2.000e-04> l_g_L1: 2.665e-02 l_g_distill: 1.452e-02 l_g_feature: 3.492e-03 G_loss: 4.466e-02 
25-12-04 21:08:19.647 : <epoch:321, iter:   9,000, lr:2.000e-04> l_g_L1: 2.838e-02 l_g_distill: 1.399e-02 l_g_feature: 3.227e-03 G_loss: 4.559e-02 
25-12-04 21:21:42.585 : <epoch:328, iter:   9,200, lr:2.000e-04> l_g_L1: 3.403e-02 l_g_distill: 1.762e-02 l_g_feature: 3.155e-03 G_loss: 5.481e-02 
25-12-04 21:35:27.941 : <epoch:335, iter:   9,400, lr:2.000e-04> l_g_L1: 2.453e-02 l_g_distill: 1.328e-02 l_g_feature: 3.791e-03 G_loss: 4.160e-02 
25-12-04 21:46:38.310 : <epoch:342, iter:   9,600, lr:2.000e-04> l_g_L1: 2.590e-02 l_g_distill: 1.162e-02 l_g_feature: 2.876e-03 G_loss: 4.040e-02 
25-12-04 21:57:23.086 : <epoch:349, iter:   9,800, lr:2.000e-04> l_g_L1: 2.712e-02 l_g_distill: 1.380e-02 l_g_feature: 2.714e-03 G_loss: 4.364e-02 
25-12-04 22:08:09.256 : <epoch:357, iter:  10,000, lr:5.000e-05> l_g_L1: 3.075e-02 l_g_distill: 1.799e-02 l_g_feature: 3.635e-03 G_loss: 5.237e-02 
25-12-04 22:08:09.258 : Saving the model.
25-12-04 22:08:12.497 : ---1--> babyx4.png | 31.41dB
25-12-04 22:08:12.575 : ---2--> birdx4.png | 29.52dB
25-12-04 22:08:12.616 : ---3--> butterflyx4.png | 22.82dB
25-12-04 22:08:12.673 : ---4--> headx4.png | 28.97dB
25-12-04 22:08:12.733 : ---5--> womanx4.png | 26.90dB
25-12-04 22:08:13.147 : <epoch:357, iter:  10,000, Average PSNR : 27.92dB

25-12-04 22:19:15.153 : <epoch:364, iter:  10,200, lr:1.000e-04> l_g_L1: 3.847e-02 l_g_distill: 2.049e-02 l_g_feature: 2.301e-03 G_loss: 6.126e-02 
25-12-04 22:29:59.240 : <epoch:371, iter:  10,400, lr:1.000e-04> l_g_L1: 2.115e-02 l_g_distill: 9.817e-03 l_g_feature: 1.760e-03 G_loss: 3.272e-02 
25-12-04 22:40:44.433 : <epoch:378, iter:  10,600, lr:1.000e-04> l_g_L1: 2.203e-02 l_g_distill: 1.222e-02 l_g_feature: 2.178e-03 G_loss: 3.643e-02 
25-12-04 22:51:43.628 : <epoch:385, iter:  10,800, lr:1.000e-04> l_g_L1: 2.566e-02 l_g_distill: 1.351e-02 l_g_feature: 1.980e-03 G_loss: 4.115e-02 
25-12-04 23:02:55.187 : <epoch:392, iter:  11,000, lr:1.000e-04> l_g_L1: 2.107e-02 l_g_distill: 1.134e-02 l_g_feature: 1.792e-03 G_loss: 3.421e-02 
25-12-04 23:13:42.772 : <epoch:399, iter:  11,200, lr:1.000e-04> l_g_L1: 3.373e-02 l_g_distill: 1.554e-02 l_g_feature: 2.149e-03 G_loss: 5.142e-02 
25-12-04 23:25:17.188 : <epoch:407, iter:  11,400, lr:1.000e-04> l_g_L1: 3.217e-02 l_g_distill: 2.012e-02 l_g_feature: 2.197e-03 G_loss: 5.449e-02 
25-12-04 23:36:54.716 : <epoch:414, iter:  11,600, lr:1.000e-04> l_g_L1: 2.400e-02 l_g_distill: 1.140e-02 l_g_feature: 1.988e-03 G_loss: 3.740e-02 
25-12-04 23:49:31.673 : <epoch:421, iter:  11,800, lr:1.000e-04> l_g_L1: 2.403e-02 l_g_distill: 1.439e-02 l_g_feature: 1.822e-03 G_loss: 4.024e-02 
25-12-05 00:01:17.850 : <epoch:428, iter:  12,000, lr:1.000e-04> l_g_L1: 2.467e-02 l_g_distill: 1.296e-02 l_g_feature: 1.942e-03 G_loss: 3.957e-02 
25-12-05 00:01:17.850 : Saving the model.
25-12-05 00:01:21.795 : ---1--> babyx4.png | 31.83dB
25-12-05 00:01:21.879 : ---2--> birdx4.png | 30.07dB
25-12-05 00:01:21.922 : ---3--> butterflyx4.png | 23.09dB
25-12-05 00:01:21.985 : ---4--> headx4.png | 29.48dB
25-12-05 00:01:22.054 : ---5--> womanx4.png | 27.35dB
25-12-05 00:01:22.433 : <epoch:428, iter:  12,000, Average PSNR : 28.36dB

25-12-05 00:14:27.598 : <epoch:435, iter:  12,200, lr:1.000e-04> l_g_L1: 3.046e-02 l_g_distill: 1.784e-02 l_g_feature: 2.338e-03 G_loss: 5.064e-02 
25-12-05 00:27:43.337 : <epoch:442, iter:  12,400, lr:1.000e-04> l_g_L1: 2.948e-02 l_g_distill: 1.440e-02 l_g_feature: 2.006e-03 G_loss: 4.589e-02 
25-12-05 00:40:29.251 : <epoch:449, iter:  12,600, lr:1.000e-04> l_g_L1: 2.738e-02 l_g_distill: 1.233e-02 l_g_feature: 1.820e-03 G_loss: 4.153e-02 
25-12-05 00:53:27.821 : <epoch:457, iter:  12,800, lr:1.000e-04> l_g_L1: 2.261e-02 l_g_distill: 1.291e-02 l_g_feature: 1.918e-03 G_loss: 3.743e-02 
25-12-05 01:06:15.194 : <epoch:464, iter:  13,000, lr:1.000e-04> l_g_L1: 2.279e-02 l_g_distill: 1.030e-02 l_g_feature: 1.685e-03 G_loss: 3.478e-02 
25-12-05 01:19:11.947 : <epoch:471, iter:  13,200, lr:1.000e-04> l_g_L1: 3.112e-02 l_g_distill: 1.352e-02 l_g_feature: 1.709e-03 G_loss: 4.636e-02 
25-12-05 01:30:57.685 : <epoch:478, iter:  13,400, lr:1.000e-04> l_g_L1: 2.119e-02 l_g_distill: 1.041e-02 l_g_feature: 1.537e-03 G_loss: 3.314e-02 
25-12-05 01:41:04.456 : <epoch:485, iter:  13,600, lr:1.000e-04> l_g_L1: 2.490e-02 l_g_distill: 1.278e-02 l_g_feature: 2.159e-03 G_loss: 3.984e-02 
25-12-05 01:51:10.714 : <epoch:492, iter:  13,800, lr:1.000e-04> l_g_L1: 2.433e-02 l_g_distill: 1.248e-02 l_g_feature: 1.921e-03 G_loss: 3.873e-02 
25-12-05 02:01:16.308 : <epoch:499, iter:  14,000, lr:1.000e-04> l_g_L1: 3.206e-02 l_g_distill: 1.547e-02 l_g_feature: 1.626e-03 G_loss: 4.915e-02 
25-12-05 02:01:16.310 : Saving the model.
25-12-05 02:01:19.346 : ---1--> babyx4.png | 31.63dB
25-12-05 02:01:19.427 : ---2--> birdx4.png | 30.15dB
25-12-05 02:01:19.463 : ---3--> butterflyx4.png | 23.34dB
25-12-05 02:01:19.513 : ---4--> headx4.png | 29.45dB
25-12-05 02:01:19.563 : ---5--> womanx4.png | 27.48dB
25-12-05 02:01:19.916 : <epoch:499, iter:  14,000, Average PSNR : 28.41dB

25-12-05 02:11:25.087 : <epoch:507, iter:  14,200, lr:1.000e-04> l_g_L1: 2.342e-02 l_g_distill: 1.319e-02 l_g_feature: 1.825e-03 G_loss: 3.843e-02 
25-12-05 02:21:30.329 : <epoch:514, iter:  14,400, lr:1.000e-04> l_g_L1: 2.731e-02 l_g_distill: 1.407e-02 l_g_feature: 1.753e-03 G_loss: 4.313e-02 
25-12-05 02:31:36.310 : <epoch:521, iter:  14,600, lr:1.000e-04> l_g_L1: 2.287e-02 l_g_distill: 1.024e-02 l_g_feature: 1.561e-03 G_loss: 3.467e-02 
25-12-05 02:41:41.935 : <epoch:528, iter:  14,800, lr:1.000e-04> l_g_L1: 2.581e-02 l_g_distill: 1.295e-02 l_g_feature: 1.964e-03 G_loss: 4.072e-02 
25-12-05 02:51:46.907 : <epoch:535, iter:  15,000, lr:1.000e-04> l_g_L1: 2.792e-02 l_g_distill: 1.646e-02 l_g_feature: 8.344e-03 G_loss: 5.272e-02 
25-12-05 03:01:51.087 : <epoch:542, iter:  15,200, lr:1.000e-04> l_g_L1: 2.506e-02 l_g_distill: 1.200e-02 l_g_feature: 2.432e-03 G_loss: 3.949e-02 
25-12-05 03:11:55.725 : <epoch:549, iter:  15,400, lr:1.000e-04> l_g_L1: 2.700e-02 l_g_distill: 1.583e-02 l_g_feature: 2.208e-03 G_loss: 4.504e-02 
25-12-05 03:22:01.349 : <epoch:557, iter:  15,600, lr:1.000e-04> l_g_L1: 2.603e-02 l_g_distill: 1.443e-02 l_g_feature: 1.965e-03 G_loss: 4.243e-02 
25-12-05 03:32:05.895 : <epoch:564, iter:  15,800, lr:1.000e-04> l_g_L1: 2.434e-02 l_g_distill: 1.034e-02 l_g_feature: 1.612e-03 G_loss: 3.629e-02 
25-12-05 03:42:13.320 : <epoch:571, iter:  16,000, lr:2.500e-05> l_g_L1: 2.466e-02 l_g_distill: 1.214e-02 l_g_feature: 1.554e-03 G_loss: 3.835e-02 
25-12-05 03:42:13.320 : Saving the model.
25-12-05 03:42:16.233 : ---1--> babyx4.png | 31.82dB
25-12-05 03:42:16.304 : ---2--> birdx4.png | 30.16dB
25-12-05 03:42:16.338 : ---3--> butterflyx4.png | 23.51dB
25-12-05 03:42:16.387 : ---4--> headx4.png | 29.50dB
25-12-05 03:42:16.439 : ---5--> womanx4.png | 27.62dB
25-12-05 03:42:16.772 : <epoch:571, iter:  16,000, Average PSNR : 28.52dB

25-12-05 03:52:23.470 : <epoch:578, iter:  16,200, lr:5.000e-05> l_g_L1: 2.816e-02 l_g_distill: 1.279e-02 l_g_feature: 1.530e-03 G_loss: 4.248e-02 
25-12-05 04:02:30.293 : <epoch:585, iter:  16,400, lr:5.000e-05> l_g_L1: 2.711e-02 l_g_distill: 1.406e-02 l_g_feature: 1.482e-03 G_loss: 4.265e-02 
25-12-05 04:12:37.162 : <epoch:592, iter:  16,600, lr:5.000e-05> l_g_L1: 3.303e-02 l_g_distill: 1.496e-02 l_g_feature: 1.503e-03 G_loss: 4.949e-02 
25-12-05 04:22:42.548 : <epoch:599, iter:  16,800, lr:5.000e-05> l_g_L1: 2.991e-02 l_g_distill: 1.637e-02 l_g_feature: 1.761e-03 G_loss: 4.804e-02 
25-12-05 04:32:48.548 : <epoch:607, iter:  17,000, lr:5.000e-05> l_g_L1: 2.838e-02 l_g_distill: 1.392e-02 l_g_feature: 1.474e-03 G_loss: 4.377e-02 
25-12-05 04:42:53.978 : <epoch:614, iter:  17,200, lr:5.000e-05> l_g_L1: 2.926e-02 l_g_distill: 1.456e-02 l_g_feature: 1.533e-03 G_loss: 4.535e-02 
25-12-05 04:52:59.371 : <epoch:621, iter:  17,400, lr:5.000e-05> l_g_L1: 2.615e-02 l_g_distill: 1.215e-02 l_g_feature: 1.299e-03 G_loss: 3.960e-02 
25-12-05 05:03:05.735 : <epoch:628, iter:  17,600, lr:5.000e-05> l_g_L1: 2.743e-02 l_g_distill: 1.311e-02 l_g_feature: 1.412e-03 G_loss: 4.195e-02 
25-12-05 05:13:12.794 : <epoch:635, iter:  17,800, lr:5.000e-05> l_g_L1: 2.402e-02 l_g_distill: 9.293e-03 l_g_feature: 1.210e-03 G_loss: 3.452e-02 
25-12-05 05:23:45.376 : <epoch:642, iter:  18,000, lr:1.250e-05> l_g_L1: 2.191e-02 l_g_distill: 1.022e-02 l_g_feature: 1.355e-03 G_loss: 3.349e-02 
25-12-05 05:23:45.376 : Saving the model.
25-12-05 05:23:48.283 : ---1--> babyx4.png | 31.87dB
25-12-05 05:23:48.337 : ---2--> birdx4.png | 30.31dB
25-12-05 05:23:48.376 : ---3--> butterflyx4.png | 23.64dB
25-12-05 05:23:48.426 : ---4--> headx4.png | 29.54dB
25-12-05 05:23:48.479 : ---5--> womanx4.png | 27.74dB
25-12-05 05:23:48.830 : <epoch:642, iter:  18,000, Average PSNR : 28.62dB

25-12-05 05:33:54.739 : <epoch:649, iter:  18,200, lr:2.500e-05> l_g_L1: 2.541e-02 l_g_distill: 1.273e-02 l_g_feature: 1.266e-03 G_loss: 3.941e-02 
25-12-05 05:44:06.388 : <epoch:657, iter:  18,400, lr:2.500e-05> l_g_L1: 2.626e-02 l_g_distill: 1.416e-02 l_g_feature: 1.306e-03 G_loss: 4.172e-02 
25-12-05 05:54:20.539 : <epoch:664, iter:  18,600, lr:2.500e-05> l_g_L1: 2.807e-02 l_g_distill: 1.103e-02 l_g_feature: 1.124e-03 G_loss: 4.022e-02 
25-12-05 06:04:24.171 : <epoch:671, iter:  18,800, lr:2.500e-05> l_g_L1: 3.442e-02 l_g_distill: 1.587e-02 l_g_feature: 1.448e-03 G_loss: 5.173e-02 
25-12-05 06:14:34.166 : <epoch:678, iter:  19,000, lr:6.250e-06> l_g_L1: 2.499e-02 l_g_distill: 1.250e-02 l_g_feature: 1.216e-03 G_loss: 3.871e-02 
25-12-05 06:24:38.375 : <epoch:685, iter:  19,200, lr:1.250e-05> l_g_L1: 2.244e-02 l_g_distill: 9.976e-03 l_g_feature: 1.068e-03 G_loss: 3.348e-02 
25-12-05 06:34:43.581 : <epoch:692, iter:  19,400, lr:1.250e-05> l_g_L1: 2.150e-02 l_g_distill: 9.414e-03 l_g_feature: 1.110e-03 G_loss: 3.202e-02 
25-12-05 06:44:47.960 : <epoch:699, iter:  19,600, lr:1.250e-05> l_g_L1: 3.233e-02 l_g_distill: 1.684e-02 l_g_feature: 1.269e-03 G_loss: 5.043e-02 
25-12-05 06:54:55.012 : <epoch:707, iter:  19,800, lr:1.250e-05> l_g_L1: 3.109e-02 l_g_distill: 1.607e-02 l_g_feature: 1.440e-03 G_loss: 4.861e-02 
25-12-05 07:05:04.988 : <epoch:714, iter:  20,000, lr:1.250e-05> l_g_L1: 2.569e-02 l_g_distill: 1.360e-02 l_g_feature: 1.283e-03 G_loss: 4.057e-02 
25-12-05 07:05:04.988 : Saving the model.
25-12-05 07:05:07.902 : ---1--> babyx4.png | 31.88dB
25-12-05 07:05:07.963 : ---2--> birdx4.png | 30.33dB
25-12-05 07:05:07.998 : ---3--> butterflyx4.png | 23.68dB
25-12-05 07:05:08.051 : ---4--> headx4.png | 29.56dB
25-12-05 07:05:08.102 : ---5--> womanx4.png | 27.75dB
25-12-05 07:05:08.457 : <epoch:714, iter:  20,000, Average PSNR : 28.64dB

25-12-05 07:15:14.156 : <epoch:721, iter:  20,200, lr:1.250e-05> l_g_L1: 2.518e-02 l_g_distill: 1.257e-02 l_g_feature: 1.203e-03 G_loss: 3.896e-02 
25-12-05 07:25:18.751 : <epoch:728, iter:  20,400, lr:1.250e-05> l_g_L1: 2.395e-02 l_g_distill: 1.133e-02 l_g_feature: 1.141e-03 G_loss: 3.642e-02 
25-12-05 07:35:23.135 : <epoch:735, iter:  20,600, lr:1.250e-05> l_g_L1: 2.924e-02 l_g_distill: 1.348e-02 l_g_feature: 1.232e-03 G_loss: 4.396e-02 
25-12-05 07:45:27.744 : <epoch:742, iter:  20,800, lr:1.250e-05> l_g_L1: 2.751e-02 l_g_distill: 1.445e-02 l_g_feature: 1.153e-03 G_loss: 4.312e-02 
25-12-05 07:55:32.575 : <epoch:749, iter:  21,000, lr:1.250e-05> l_g_L1: 1.945e-02 l_g_distill: 1.016e-02 l_g_feature: 1.089e-03 G_loss: 3.071e-02 
25-12-05 08:05:37.596 : <epoch:757, iter:  21,200, lr:1.250e-05> l_g_L1: 2.638e-02 l_g_distill: 1.354e-02 l_g_feature: 1.179e-03 G_loss: 4.110e-02 
25-12-05 08:15:43.430 : <epoch:764, iter:  21,400, lr:1.250e-05> l_g_L1: 2.956e-02 l_g_distill: 1.295e-02 l_g_feature: 1.162e-03 G_loss: 4.368e-02 
25-12-05 08:25:48.977 : <epoch:771, iter:  21,600, lr:1.250e-05> l_g_L1: 2.336e-02 l_g_distill: 1.095e-02 l_g_feature: 1.170e-03 G_loss: 3.548e-02 
25-12-05 08:36:10.431 : <epoch:778, iter:  21,800, lr:1.250e-05> l_g_L1: 2.657e-02 l_g_distill: 1.074e-02 l_g_feature: 1.100e-03 G_loss: 3.841e-02 
25-12-05 08:46:21.032 : <epoch:785, iter:  22,000, lr:1.250e-05> l_g_L1: 2.703e-02 l_g_distill: 1.240e-02 l_g_feature: 1.126e-03 G_loss: 4.056e-02 
25-12-05 08:46:21.032 : Saving the model.
25-12-05 08:46:24.043 : ---1--> babyx4.png | 31.89dB
25-12-05 08:46:24.119 : ---2--> birdx4.png | 30.34dB
25-12-05 08:46:24.170 : ---3--> butterflyx4.png | 23.69dB
25-12-05 08:46:24.221 : ---4--> headx4.png | 29.55dB
25-12-05 08:46:24.274 : ---5--> womanx4.png | 27.74dB
25-12-05 08:46:24.619 : <epoch:785, iter:  22,000, Average PSNR : 28.64dB

25-12-05 08:56:32.219 : <epoch:792, iter:  22,200, lr:1.250e-05> l_g_L1: 2.008e-02 l_g_distill: 1.010e-02 l_g_feature: 9.632e-04 G_loss: 3.114e-02 
25-12-05 09:06:40.036 : <epoch:799, iter:  22,400, lr:1.250e-05> l_g_L1: 2.166e-02 l_g_distill: 9.629e-03 l_g_feature: 1.202e-03 G_loss: 3.249e-02 
25-12-05 09:16:48.785 : <epoch:807, iter:  22,600, lr:1.250e-05> l_g_L1: 2.316e-02 l_g_distill: 9.843e-03 l_g_feature: 1.052e-03 G_loss: 3.406e-02 
25-12-05 09:26:57.434 : <epoch:814, iter:  22,800, lr:1.250e-05> l_g_L1: 3.179e-02 l_g_distill: 1.211e-02 l_g_feature: 1.197e-03 G_loss: 4.510e-02 
25-12-05 09:37:05.058 : <epoch:821, iter:  23,000, lr:1.250e-05> l_g_L1: 2.052e-02 l_g_distill: 9.447e-03 l_g_feature: 1.001e-03 G_loss: 3.097e-02 
25-12-05 09:47:10.840 : <epoch:828, iter:  23,200, lr:1.250e-05> l_g_L1: 2.491e-02 l_g_distill: 1.196e-02 l_g_feature: 1.072e-03 G_loss: 3.795e-02 
25-12-05 09:57:15.833 : <epoch:835, iter:  23,400, lr:1.250e-05> l_g_L1: 2.950e-02 l_g_distill: 1.598e-02 l_g_feature: 1.211e-03 G_loss: 4.669e-02 
